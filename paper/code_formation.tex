\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{float}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% Custom colors
\definecolor{critical}{RGB}{213,94,0}
\definecolor{control}{RGB}{0,158,115}
\definecolor{highk}{RGB}{0,114,178}

% Title
\title{Discrete Codes from Continuous Substrates:\\
A Minimal Channel-Capacity Simulation}

\author{Ian Todd\\
\textit{University of Sydney}\\
\texttt{itod2305@uni.sydney.edu.au}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a minimal simulation demonstrating that discrete symbolic codes emerge spontaneously when continuous data passes through a noisy bandwidth-limited channel. Using a simple autoencoder trained on a 1-D ring manifold embedded in 512-D, we show that neither the dimensional bottleneck nor channel noise alone produces discretization---both ingredients are required for the qualitative transition from continuous to discrete representation. We quantify this transition using the Adjusted Rand Index (semantic preservation) and participation ratio (effective dimensionality). The simulation provides a concrete, reproducible demonstration of how discrete codes can emerge from continuous substrates through information-theoretic constraints, with potential relevance to categorical perception, neural coding, and the emergence of digital information in biological systems.
\end{abstract}

\section{Introduction}

How do discrete symbols emerge from continuous physical substrates? This question spans cognitive science (how do brains form discrete concepts?), linguistics (how did discrete language emerge?), and origin-of-life research (how did digital genetic information arise from analog chemistry?).

We approach this question computationally, constructing a minimal simulation that exhibits spontaneous discretization. The goal is not to model any specific biological system, but to identify the \textit{sufficient conditions} for code formation---the minimal ingredients that, when combined, produce discrete representations.

Our central finding is that two ingredients are required:
\begin{enumerate}
    \item A \textbf{dimensional bottleneck}: Information must pass through a channel with fewer degrees of freedom than the input
    \item \textbf{Channel noise}: The transmission is corrupted by stochastic perturbations
\end{enumerate}

Neither alone produces discretization. The bottleneck without noise preserves continuous structure. Noise without a bottleneck leaves room for continuous codes. But together, they force the system into a regime where only discrete codes can reliably transmit information.

This paper describes what we simulated, how we simulated it, and what it might mean.

%% Physics translation table
\begin{table}[H]
\centering
\caption{Mapping between simulation components and physics concepts.}
\label{tab:rosetta}
\begin{tabular}{lll}
\toprule
\textbf{Simulation} & \textbf{Physics Analogue} & \textbf{Mathematical Proxy} \\
\midrule
Input data & Continuous manifold & $S^1$ with $O(2)$ symmetry \\
Bottleneck ($k$) & Degrees of freedom & Embedding dimension \\
Noise ($\sigma$) & Thermal fluctuations & Temperature-like parameter \\
Loss function & Energy-like objective & Effective potential to minimize \\
Code clustering & Symmetry reduction & $O(2) \to C_n$ \\
\bottomrule
\end{tabular}
\end{table}

\section{What We Simulated}

\subsection{The Input: A Continuous Manifold}

We need a continuous input space with clear topology. We chose a ring ($S^1$)---a 1-dimensional closed manifold---embedded in high-dimensional space:

\begin{lstlisting}
# Generate 2000 points uniformly on a circle
theta = random(0, 2*pi, n_samples=2000)
ring_2d = [cos(theta), sin(theta)]

# Embed in 512 dimensions via random projection
projection = random_orthonormal(2, 512)
data_512d = ring_2d @ projection + noise(0.05)
\end{lstlisting}

The embedding noise (0.05) is small relative to the ring radius, so the intrinsic $S^1$ topology is preserved. The input is \textit{continuous}---points can take any value along the ring. There are no discrete categories in the data itself.

For \textit{evaluation only}, we partition the ring into 6 equal sectors and ask: do the learned codes preserve this structure? We treat these sectors as ``semantic classes'' and measure the Adjusted Rand Index (ARI) between k-means clusters of learned codes and ground-truth sectors.

\subsection{The Channel: Encoder-Noise-Decoder}

The core architecture is a sender-channel-receiver system:

\begin{lstlisting}
# Encoder: compress 512D to k dimensions
code = encoder(input)  # code is k-dimensional

# Channel: add Gaussian noise
noisy_code = code + noise(sigma)

# Decoder: reconstruct 512D from noisy code
output = decoder(noisy_code)

# Training objective: minimize reconstruction error
loss = mean_squared_error(input, output)
\end{lstlisting}

The encoder and decoder are neural networks (3-layer MLPs with ReLU activations). The channel dimension $k$ and noise level $\sigma$ are the experimental variables.

The key insight is that the encoder must learn codes that are \textit{robust to noise}. If two nearby codes get confused after noise corruption, the decoder cannot distinguish them, and reconstruction suffers. The only solution is to space codes apart---creating discrete clusters with ``safety margins.''

\subsection{The Metrics}

We measure two quantities:

\textbf{Semantic Preservation (ARI)}: We cluster learned codes via k-means ($n=6$) and compute the Adjusted Rand Index against the 6-sector ground truth. ARI $= 1$ means perfect cluster-label agreement; ARI $= 0$ means agreement no better than chance.

\textbf{Effective Dimensionality (Participation Ratio)}:
\begin{equation}
    \text{PR} = \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2}
\end{equation}
where $\lambda_i$ are eigenvalues of the code covariance matrix. PR $= 1$ for data collapsed to a line; PR $\approx d$ for variance spread evenly across $d$ orthogonal directions. This serves as a coarse measure of effective embedding dimension.

\section{The Control Experiment}

Our main result comes from a simple control experiment. We ask: what \textit{causes} the discretization?

\begin{table}[H]
\centering
\caption{Control experiment: both bottleneck AND noise required for clustering.}
\label{tab:control}
\begin{tabular}{lccc}
\toprule
Condition & Bottleneck ($k$) & Noise ($\sigma$) & Result \\
\midrule
Bottleneck only & 2 & 0 & \textcolor{control}{Continuous ring} \\
Noise only & 32 & 0.5 & \textcolor{highk}{Continuous ring} \\
\textbf{Both} & \textbf{2} & \textbf{0.5} & \textcolor{critical}{\textbf{Discrete clusters}} \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:control} shows this visually. The top row tells a linear story:
\begin{enumerate}
    \item \textbf{The Input}: A perfect continuous ring (color = angular position $\theta$)
    \item \textbf{Bottleneck Only}: Compressed to $k=2$ with no noise---ring topology preserved
    \item \textbf{Bottleneck + Noise}: Compressed to $k=2$ with $\sigma=0.5$---discrete clusters emerge
\end{enumerate}

The bottom row shows that increasing $k$ (with noise held constant) restores continuous representation. At $k=32$, there is enough ``room'' in the channel for the ring to survive despite noise.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../output/codes_by_channel_dim.png}
\caption{\textbf{Visual proof of the two-ingredient hypothesis.} Top row: (1) Continuous input ring, (2) bottleneck alone preserves topology, (3) bottleneck + noise $\to$ discrete clusters. Black X's mark learned centroids (k-means). Bottom row: As channel dimension $k$ increases, ring structure returns despite noise ($\sigma = 0.5$ throughout).}
\label{fig:control}
\end{figure}

\section{The Transition}

Figure~\ref{fig:phase} shows how semantic preservation varies with channel dimension $k$. There are three regimes:

\begin{enumerate}
    \item \textbf{$k=1$: Topological collapse.} The ring is forced onto a line. Adjacent points on the ring may map to distant points on the line. The $S^1$ topology is destroyed.

    \item \textbf{$k \approx k_c$: Critical capacity.} Discrete codes emerge that optimally preserve semantic structure. The system ``discovers'' that a small number of well-separated clusters is the best way to use the limited channel.

    \item \textbf{$k \gg k_c$: Continuous preservation.} With sufficient bandwidth, the ring can be transmitted continuously. Noise is present but doesn't force discretization.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/summary_figure.png}
\caption{\textbf{Qualitative transition in code structure.} (a) Schematic showing three regimes. (b) Semantic preservation (ARI vs.~6-sector ground truth) as function of channel dimension $k$. Peak near $k_c \approx 2$. Noise $\sigma = 0.5$ throughout.}
\label{fig:phase}
\end{figure}

In the learned representation, there is a clear topology-changing transition: for $k=1$ the ring maps to a line, at intermediate $k$ to well-separated clusters, and for large $k$ back to a continuous ring-like manifold. In the language of symmetry, this corresponds to a reduction from the continuous $O(2)$ rotational symmetry of the input ring to a discrete $C_n$ point-group symmetry in code space.

We can define an order parameter for this transition as the silhouette score of the code distribution, or equivalently the ARI with respect to equal-sector partitions. This order parameter is near zero for $k=1$ (collapsed) and $k \gg k_c$ (continuous), and peaks at intermediate $k$ where discrete clustering is strongest.

\textit{Note:} We use ``transition'' rather than ``phase transition'' advisedly. In a strict thermodynamic sense, phase transitions occur in the infinite-size limit where singularities appear in the free energy. Our finite neural network exhibits a sharp crossover or bifurcation, not a true thermodynamic phase transition. The symmetry-breaking language describes the \textit{representation-level} structure, not a thermodynamic equilibrium.

\section{The Mechanism: Capacity Constraints}

Why does discretization occur? The intuition comes from \textbf{sphere packing} and \textbf{channel capacity}.

\subsection{Geometric Picture}

Channel noise smears each code point into an ``uncertainty region'' of characteristic scale $\sim\sigma$. If two codes are closer than this scale, their uncertainty regions overlap, and the decoder cannot reliably distinguish them.

The question becomes: how many well-separated codes can coexist in a $k$-dimensional channel? This is related to the classical sphere-packing problem \citep{conway1999}. For fixed noise scale and code amplitude, only a finite number of non-overlapping regions fit in low-dimensional space.

\subsection{Information-Theoretic Picture}

The Shannon--Hartley theorem \citep{shannon1948} gives the capacity of an additive white Gaussian noise (AWGN) channel:
\begin{equation}
    C \propto k \cdot \log_2(1 + \text{SNR})
\end{equation}
where $k$ is the number of channel uses (dimensions) and SNR is the signal-to-noise ratio. The \textit{logarithm} of the number of distinguishable codewords scales with this capacity.

We do not attempt a precise calibration here. The point is qualitative: for fixed $k$ and $\sigma$, capacity is finite, so only a finite number of well-separated codes can coexist. For our parameters ($k=2$, $\sigma=0.5$), we empirically observe $\sim$6 clusters. We do not claim the theory uniquely predicts ``6''---only that a small integer number of clusters is natural in this regime.

The network doesn't ``know'' about sphere packing or Shannon capacity; it discovers these constraints through gradient descent on reconstruction error.

\section{What Gets Preserved}

When compressing from 512 dimensions to $k$ dimensions, what does the encoder choose to preserve?

Figure~\ref{fig:pca} shows that the encoder learns to preserve \textbf{high-variance directions}---essentially the top principal components of the input. The ring lives primarily in the first 2 PCs (we verified that PC1 $\approx \cos\theta$ and PC2 $\approx \sin\theta$ by plotting against $\theta$), and these are what the encoder captures.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../output/what_gets_preserved.png}
\caption{\textbf{What gets preserved.} Top: Input lives in first 2 PCs (intrinsic dimensionality $\approx 2$). PC1-2 capture the ring; higher PCs are embedding noise. Bottom: Encoder learns to preserve these high-variance directions. At $k=2$ with noise, discretization occurs.}
\label{fig:pca}
\end{figure}

This makes intuitive sense: high-variance directions carry the most information about input structure. Low-variance directions (the remaining 510 dimensions of embedding noise) are discarded as uninformative.

In the limit of a linear encoder/decoder and small noise, the optimum is PCA. Here the nonlinearity (ReLU) allows genuine topology changes---the ``tearing'' of the continuous ring into discrete clusters when capacity is insufficient.

\section{Extension: Stochastic Resonance}

Our framework provides a geometric interpretation of the \textbf{stochastic resonance} (SR) phenomenon---where adding noise can \textit{improve} signal detection \citep{gammaitoni1998}.

In the classical SR picture, a subthreshold periodic signal in a bistable potential is optimally detected at intermediate noise, which enables barrier crossing. Here, the ``potential wells'' are code basins learned by the autoencoder, and ambiguity arises from signals lying near decision boundaries.

Consider an ambiguous signal sitting exactly on the boundary between two code clusters. In low-noise conditions, the dynamics are confined to the low-dimensional manifold of learned codes---the signal is ``trapped'' on the decision boundary.

Adding noise allows the system to explore the ambient high-dimensional embedding space. The signal can fluctuate perpendicular to the boundary, enabling escape into the correct attractor basin by moving \textit{around} the confusion region in high-D space rather than being stuck on a low-D saddle.

Figure~\ref{fig:sr} demonstrates this SR-like behavior:
\begin{itemize}
    \item At zero noise, ambiguous signals are trapped on boundaries $\to$ poor decoding
    \item At intermediate noise, the system can escape to correct basins $\to$ peak accuracy
    \item At high noise, random walk dominates $\to$ decoding degrades
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../output/stochastic_resonance.png}
\caption{\textbf{SR-like behavior in code decoding.} (a) Learned codes and ambiguous test signals on basin boundaries. (b) Accuracy peaks at intermediate noise---the hallmark SR curve. (c) Effective dimensionality (PR) increases monotonically with noise. (d--f) Geometric interpretation: noise enables exploration of ambient space to escape low-D traps.}
\label{fig:sr}
\end{figure}

This reframes SR in geometric terms: noise provides access to extra degrees of freedom in the ambient embedding space, enabling escape from low-dimensional traps. Optimal performance occurs when noise is sufficient to escape boundaries but not so large as to cause random wandering.

\section{Speculative Implications}

The simulation is deliberately minimal and agnostic about biophysical details. The points below are \textit{speculative connections}, not claims that this specific architecture underlies any particular system.

\subsection{Categorical Perception}

Humans perceive continuous stimuli as discrete categories---colors, phonemes, facial expressions \citep{harnad1987}. If sensory information must pass through bandwidth-limited neural pathways (due to metabolic constraints, synaptic noise, or architectural bottlenecks), discretization of the kind demonstrated here could contribute to categorical perception.

\subsection{Ephaptic-to-Neural Coupling}

A speculative but intriguing application concerns the interface between ephaptic fields and neural spiking. Ephaptic coupling---the influence of local field potentials on nearby neurons---represents a high-dimensional, continuous electromagnetic field interacting with discrete spike-based neural codes. If information must flow from the continuous field domain into the discrete spiking domain through a capacity-limited interface (e.g., voltage-sensitive ion channels with finite dynamic range and noise), the kind of discretization demonstrated here could play a role. The ``bottleneck'' would be the biophysical coupling mechanism; the ``noise'' would be thermal and channel noise. This suggests a framework for understanding how continuous field dynamics might give rise to discrete neural representations.

\subsection{Symbol Emergence in Language}

Discrete tokens are fundamental to language. Our simulation illustrates one mechanism by which discretization can emerge without explicit symbolic machinery---through the interaction of compression and noise. This might inform theories of language evolution, though many other factors are surely involved.

\subsection{Digital Information in Biology}

Life uses digital information (genetic sequences) despite operating in an analog chemical medium. Noise and capacity constraints may have played a role in the emergence of discrete coding in early self-replicating systems \citep{walker2017, deacon2011}, though this remains highly speculative.

\subsection{Relation to Thermodynamics}

One might ask how this relates to Landauer's principle, which bounds the energy cost of bit erasure \citep{landauer1961, bennett1982}. Our simulation has no explicit energy model, so we cannot make direct thermodynamic claims.

However, a thermodynamic extension is natural: treat the reconstruction loss as an energy-like potential $U(\mathbf{c})$ and the channel noise variance as proportional to temperature ($\sigma^2 \propto k_B T$). The system then minimizes an effective free energy $F = \langle U \rangle - TS$, where $S$ is the entropy of the code distribution. Discretization would correspond to a ``crystallization'' into low-energy, high-entropy basins when temperature (noise) is high relative to channel capacity.

In this picture, the stochastic resonance phenomenon becomes standard barrier-crossing dynamics: noise provides the thermal energy to escape local minima and explore the potential landscape. The ``information'' emerges when the system settles into a basin---the uncertainty collapses, and the basin identity carries $\log_2 n$ bits.

We have not developed this thermodynamic model here; doing so properly would require specifying the potential landscape, computing Kramers rates for basin transitions, and tracking entropy production along trajectories. This is a natural direction for future work, connecting to stochastic thermodynamics and the physics of biological information processing. For now, we simply note that the geometry demonstrated here---capacity-limited discretization, SR-like basin escape---is precisely what a thermodynamic model would need to explain.

\section{Relation to Prior Work}

The results connect to several established frameworks:

\textbf{Rate--distortion theory and vector quantization} \citep{cover2006}: Our encoder-decoder system minimizes distortion subject to a rate constraint (channel capacity). At low rates, the optimal solution involves quantization---precisely the discrete codes we observe.

\textbf{Information bottleneck} \citep{tishby2000}: The bottleneck principle frames representation learning as extracting minimal sufficient statistics. Our simulation provides a concrete visualization of what happens geometrically when the bottleneck is tight.

\textbf{What's new here}: The explicit topology-change picture (ring $\to$ line/clusters/ring), the use of PR and ARI to track geometry and semantics, and the SR-like dimensionality story.

\section{Limitations}

This simulation is deliberately minimal:
\begin{itemize}
    \item \textbf{Static data}: We use a fixed manifold, not dynamical or temporal structure
    \item \textbf{Gaussian noise}: Real channels have more complex noise statistics
    \item \textbf{Feedforward architecture}: Biological systems have recurrence and feedback
    \item \textbf{Single manifold}: The input is a simple ring; real data has more complex topology
    \item \textbf{No explicit physics}: There is no energy function or thermodynamic model
\end{itemize}

These limitations define the scope. Extending to time-series, recurrent architectures, and explicit energy models is future work.

\section{Reproducibility}

The simulation is implemented in Python using PyTorch. Code and figures are available at \url{https://github.com/todd866/code-formation}. Key parameters:
\begin{itemize}
    \item Input: 2000 points, 512 dimensions, ring manifold ($S^1$)
    \item Encoder/Decoder: 3-layer MLP, 256 hidden units, ReLU activation
    \item Training: Adam optimizer, 150 epochs, MSE loss
    \item Critical experiment: $k=2$, $\sigma=0.5$
\end{itemize}

Random seeds are fixed for reproducibility. Running \texttt{python code\_formation.py} generates all figures.

\section{Conclusion}

We have presented a minimal simulation demonstrating the emergence of discrete codes from continuous substrates when information passes through a noisy, capacity-limited channel. The key finding is that \textbf{both} a dimensional bottleneck \textbf{and} channel noise are required---neither alone suffices.

The mechanism is geometrically intuitive: noise creates uncertainty regions around each code point, and limited channel capacity means only a finite number of well-separated codes can coexist. The network discovers this constraint through gradient descent, producing discrete clusters with safety margins.

This illustrates one concrete mechanism by which symbol-like discrete codes can emerge from continuous substrates under capacity and noise constraints. Whether analogous mechanisms operate in biological systems---in perception, memory, language, or molecular information---remains an open and fascinating question.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
